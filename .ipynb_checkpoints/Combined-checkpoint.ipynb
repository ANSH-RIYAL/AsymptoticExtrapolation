{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f53f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5328f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea briefing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60d744bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc02edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target functions\n",
    "# responsible for creating the datasets on which NNs are trained and their \n",
    "# history saved as dataset for our \"Ultimate\" asymptote prediction model\n",
    "\n",
    "def targetValueFunction(inpAttr, iteration):\n",
    "    # inpAttr shape = (samples, numberOfAttributes)\n",
    "    \n",
    "    # Setting range of random noise as -0.1 to 0.1\n",
    "    noise = np.random.random_sample(inpAttr.shape[1])\n",
    "    noise = 0.2*noise - 0.1\n",
    "    # Function 1\n",
    "    t = cFunc1(inpAttr, noise)\n",
    "    t = np.expand_dims(t, axis=1)\n",
    "    t1 = np.concatenate((inpAttr,t), axis = 1)\n",
    "    with open('./data/function1_{}_values.npy'.format(iteration), 'wb') as f:\n",
    "        np.save(f, np.asarray(t1))\n",
    "    \n",
    "    \n",
    "    noise = np.random.random_sample(inpAttr.shape[1])\n",
    "    noise = 0.2*noise - 0.1\n",
    "    # Function 2\n",
    "    t = cFunc2(inpAttr, noise)\n",
    "    t = np.expand_dims(t, axis=1)\n",
    "    t2 = np.concatenate((inpAttr,t), axis = 1)\n",
    "    with open('./data/function2_{}_values.npy'.format(iteration), 'wb') as f:\n",
    "        np.save(f, np.asarray(t2))\n",
    "    \n",
    "    \n",
    "    noise = np.random.random_sample(inpAttr.shape[1])\n",
    "    noise = 0.2*noise - 0.1\n",
    "    # Function 3\n",
    "    t = cFunc3(inpAttr, noise)\n",
    "    t = np.expand_dims(t, axis=1)\n",
    "    t3 = np.concatenate((inpAttr,t), axis = 1)\n",
    "    with open('./data/function3_{}_values.npy'.format(iteration), 'wb') as f:\n",
    "        np.save(f, np.asarray(t3))\n",
    "    \n",
    "    \n",
    "    noise = np.random.random_sample(inpAttr.shape[1])\n",
    "    noise = 0.2*noise - 0.1\n",
    "    # Function 4\n",
    "    t = cFunc4(inpAttr, noise)\n",
    "    t = np.expand_dims(t, axis=1)\n",
    "    t4 = np.concatenate((inpAttr,t), axis = 1)\n",
    "    with open('./data/function4_{}_values.npy'.format(iteration), 'wb') as f:\n",
    "        np.save(f, np.asarray(t4))\n",
    "    \n",
    "    \n",
    "    noise = np.random.random_sample(inpAttr.shape[1])\n",
    "    noise = 0.2*noise - 0.1\n",
    "    # Function 5\n",
    "    t = cFunc5(inpAttr, noise)\n",
    "    t = np.expand_dims(t, axis=1)\n",
    "    t5 = np.concatenate((inpAttr,t), axis = 1)\n",
    "    with open('./data/function5_{}_values.npy'.format(iteration), 'wb') as f:\n",
    "        np.save(f, np.asarray(t5))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# cFunc 1...5 are the target functions on the basis of which the synthetic datasets are created\n",
    "def cFunc1(inpAttr, bias):\n",
    "    inpAttr = inpAttr*inpAttr*inpAttr*inpAttr\n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        for j in range(inpAttr.shape[1]):\n",
    "            inpAttr[i][j] = tanh(inpAttr[i][j])\n",
    "    \n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        for j in range(inpAttr.shape[1]):\n",
    "            inpAttr[i][j] = sqrt(inpAttr[i][j])\n",
    "    \n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        inpAttr[i] += bias\n",
    "#     returns values = sum(root(tanh(x^4)) + bias)\n",
    "    target = np.sum(inpAttr, axis = 1)\n",
    "    return target\n",
    "\n",
    "def cFunc2(inpAttr, bias):\n",
    "    inpAttr = inpAttr*inpAttr\n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        for j in range(inpAttr.shape[1]):\n",
    "            inpAttr[i][j] = tanh(inpAttr[i][j])\n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        inpAttr[i] += bias\n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        for j in range(inpAttr.shape[1]):\n",
    "            inpAttr[i][j] = sqrt(inpAttr[i][j])\n",
    "#     returns values = sum(root(tanh(x^2) + bias))\n",
    "    target = np.sum(inpAttr, axis = 1)\n",
    "    return target\n",
    "\n",
    "def cFunc3(inpAttr, bias):\n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        for j in range(inpAttr.shape[1]):\n",
    "            inpAttr[i][j] = tanh(inpAttr[i][j])\n",
    "            inpAttr[i][j] *= inpAttr[i][j]\n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        inpAttr[i] += bias\n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        for j in range(inpAttr.shape[1]):\n",
    "            inpAttr[i][j] = sqrt(inpAttr[i][j])\n",
    "#     returns values = sum(root(tanh(x)^2 + bias))\n",
    "    target = np.sum(inpAttr, axis = 1)\n",
    "    return target\n",
    "\n",
    "def cFunc4(inpAttr, bias):\n",
    "    inpAttr = inpAttr*inpAttr\n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        for j in range(inpAttr.shape[1]):\n",
    "            inpAttr[i][j] = tanh(inpAttr[i][j])\n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        inpAttr[i] += bias\n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        for j in range(inpAttr.shape[1]):\n",
    "            inpAttr[i][j] = sqrt(inpAttr[i][j])\n",
    "#     returns values = sum(root(tanh(x^2) + bias))\n",
    "    target = np.sum(inpAttr, axis = 1)\n",
    "    return target\n",
    "\n",
    "def cFunc5(inpAttr, bias):\n",
    "    inpAttr = inpAttr*inpAttr\n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        for j in range(inpAttr.shape[1]):\n",
    "            inpAttr[i][j] = tanh(inpAttr[i][j])\n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        inpAttr[i] += bias\n",
    "    for i in range(inpAttr.shape[0]):\n",
    "        for j in range(inpAttr.shape[1]):\n",
    "            inpAttr[i][j] = sqrt(inpAttr[i][j])\n",
    "#     returns values = sum(root(tanh(x^2) + bias))\n",
    "    target = np.sum(inpAttr, axis = 1)\n",
    "    return target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89669d19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b5d4e3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6313af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create database: \"Pre-Data\"\n",
    "\n",
    "numSamples = 5000\n",
    "numAttributes = 10\n",
    "for i in range(10):\n",
    "    inpX = np.random.random_sample((numSamples,numAttributes))\n",
    "    targetValueFunction(inpX,i)\n",
    "\n",
    "# Now we have already saved the datasets we needed to create \"Pre-Data\"\n",
    "# As we will observe later, the shape of each of the synthetic datasets is (numSamples, numAttributes + 1)\n",
    "# We will load these datasets later and train our models on them to get the training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb32da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6843c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network which must not be named..... jk, who's working has to be optimised\n",
    "def createExpModel():\n",
    "    experimentalModel = Sequential()\n",
    "    experimentalModel.add(Dense(2*numAttributes, input_shape = (numAttributes,), activation = tf.keras.activations.tanh))\n",
    "    experimentalModel.add(Dense(numAttributes))\n",
    "    experimentalModel.add(Dense(1))\n",
    "    experimentalModel.summary()\n",
    "    return experimentalModel\n",
    "\n",
    "models = [None, None, None, None, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93bb443",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f12180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading first set of weights\n",
    "allValues = None\n",
    "i = 1\n",
    "\n",
    "for j in range(10):\n",
    "    targets = np.load(\"./data/function{}_{}_values.npy\".format(i,j))\n",
    "    if type(allValues) == type(None):\n",
    "        allValues = np.array(targets)\n",
    "    else:\n",
    "        print(allValues.shape)\n",
    "        print(targets.shape)\n",
    "        allValues = np.concatenate((allValues,targets))\n",
    "\n",
    "print(allValues.shape)\n",
    "\n",
    "# By this, we have loaded 10 datasets for the first target function, \n",
    "# we will use these to train a single dummy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07500bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "models[i-1] = createExpModel()\n",
    "print(models[i-1].summary())\n",
    "\n",
    "models[i-1].compile(optimizer = 'adam', loss= 'mse', metrics = ['accuracy'])\n",
    "\n",
    "# Run the below line if you want to be intimidated\n",
    "# print(model_1.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e7e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, like any other ML program, we take the dataset and extract the input and target attributes\n",
    "inputValues = allValues[:,:10]\n",
    "print(inputValues.shape)\n",
    "targetValues = allValues[:,10]\n",
    "print(targetValues.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "trackingArray = []\n",
    "batch_size = 20\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    for step in range(0,inputValues.shape[0]-5,15):\n",
    "        models[i-1].fit(np.asarray(inputValues[step:step+batch_size]),np.asarray(targetValues[step:step+batch_size]))\n",
    "        trackingArray.append(np.array(models[step-1].get_weights()))\n",
    "print(len(trackingArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we export the \" Dataset\"\n",
    "\n",
    "exportedTargetValues = np.asarray(trackingArray)\n",
    "with open('./trainings/trainingValuesFunction{}.npy'.format(i), 'wb') as f:\n",
    "        np.save(f, exportedTargetValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356df97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you might be tempted to ask why I used \"i\" everywhere when the value is already hardcoded as 1\n",
    "# Right?! It's a mystery right?.... okay maybe not as big of a reveal as I was hoping....\n",
    "# Yes! you geniuses! we're going to use a loop to do all of it in a single go so that we don't have\n",
    "# to wait around clicking buttons every 5 mins... seriously, how lazy can you guys be! TSK!\n",
    "\n",
    "for i in range(1,6):\n",
    "    \n",
    "    # Dataset loading\n",
    "    allValues = None\n",
    "    for j in range(10):\n",
    "        targets = np.load(\"./data/function{}_{}_values.npy\".format(i,j))\n",
    "        if type(allValues) == type(None):\n",
    "            allValues = np.array(targets)\n",
    "        else:\n",
    "            allValues = np.concatenate((allValues,targets))\n",
    "    \n",
    "    # Model Creation and compiling\n",
    "    models[i-1] = createExpModel()\n",
    "    models[i-1].compile(optimizer = 'adam', loss= 'mse', metrics = ['accuracy'])\n",
    "    \n",
    "    # splitting attributes into input and output\n",
    "    inputValues = allValues[:,:10]\n",
    "    targetValues = allValues[:,10]\n",
    "    \n",
    "    # Training\n",
    "    trackingArray = []\n",
    "    batch_size = 20\n",
    "    epochs = 2\n",
    "    for epoch in range(epochs):\n",
    "        for step in range(0,inputValues.shape[0]-5,15):\n",
    "            models[i-1].fit(np.asarray(inputValues[step:step+batch_size]),np.asarray(targetValues[step:step+batch_size]))\n",
    "            trackingArray.append(np.array(models[step-1].get_weights()))  \n",
    "    \n",
    "    # Exporting the weights\n",
    "    exportedTargetValues = np.asarray(trackingArray)\n",
    "    with open('./trainings/trainingValuesFunction{}.npy'.format(i), 'wb') as f:\n",
    "            np.save(f, exportedTargetValues)\n",
    "\n",
    "# Yaay!! Now give the computer some time to run everything while you chill out listening to this dope playlist\n",
    "# https://www.youtube.com/watch?v=_tNU6dpjIyM&list=RDnWmuBVGkTCw&index=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"Bla Bla Dataset\", combine them into one and then do the steps given below for all of them.\n",
    "# It gives more confidence when you code by yourself (alright you got me, I'm lazy)\n",
    "# I will only do it for one dataset, if you understood everything, doing it for everything will be easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa72c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trackingArray = np.asarray(trackingArray)\n",
    "trackingArray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here things get a little too specific\n",
    "# We are gonna have to split them into individual sequences for each trainable variable.\n",
    "# To do that, we will observe the shape of the tracking array and index the values with that.\n",
    "# Just to be clear, this can also be done by reshaping the numpy array, but this gives more \n",
    "# transparency into what we are trying to do\n",
    "\n",
    "for i in range(6):\n",
    "    print(np.asarray(trackingArray[0][i]).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ugh... now comes something I like to call \"Clean Mess\"\n",
    "# So we will be first splitting all the training sequences according to the layers and biases of their nodes\n",
    "# We are doing this to observe the graph plot patterns of similarly situated elements\n",
    "\n",
    "layer1 = []\n",
    "layer2 = []\n",
    "for i in range(10*20):\n",
    "    layer1.append([])\n",
    "    layer2.append([])\n",
    "bias1 = []\n",
    "bias2 = []\n",
    "layer3 = []\n",
    "for i in range(10):\n",
    "    layer3.append([])\n",
    "    bias1.append([])\n",
    "    bias1.append([])\n",
    "    bias2.append([])\n",
    "bias3 = []\n",
    "for i in range(trackingArray.shape[0]):\n",
    "    for k in range(200):\n",
    "        layer1[k].append(trackingArray[i][0][int(k//20)][int(k%20)])\n",
    "        layer2[k].append(trackingArray[i][2][int(k//10)][int(k%10)])\n",
    "    for k in range(10):\n",
    "        layer3[k].append(trackingArray[i][4][k][0])\n",
    "        bias2[k].append(trackingArray[i][3][k])\n",
    "        bias1[2*k].append(trackingArray[i][1][2*k])\n",
    "        bias1[2*k+1].append(trackingArray[i][1][2*k+1])\n",
    "    bias3.append(trackingArray[i][5])\n",
    "\n",
    "# By the way, others call it grunt work\n",
    "\n",
    "layer1 = np.asarray(layer1)\n",
    "layer2 = np.asarray(layer2)\n",
    "layer3 = np.asarray(layer3)\n",
    "bias1 = np.asarray(bias1)\n",
    "bias2 = np.asarray(bias2)\n",
    "bias3 = np.asarray(bias3)\n",
    "\n",
    "# Just so that everyone can verify they are understanding everything correctly:\n",
    "print(layer1.shape)\n",
    "print(layer2.shape)\n",
    "print(layer3.shape)\n",
    "print(bias1.shape)\n",
    "print(bias2.shape)\n",
    "print(bias3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652682d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    plt.plot(layer1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3485a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    plt.plot(layer2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959f41ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.plot(layer3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c89de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    plt.plot(bias1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e324d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.plot(bias2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144116da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bias3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will reshape bias3 so that it can be concatenated with the rest in the following blocks of code\n",
    "bias3 = np.reshape(bias3, (bias3.shape[0]))\n",
    "print(bias3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "allLists = []\n",
    "\n",
    "for i in range(200):\n",
    "    allLists.append(layer1[i])\n",
    "    \n",
    "for i in range(200):\n",
    "    allLists.append(layer2[i])\n",
    "    \n",
    "for i in range(10):\n",
    "    allLists.append(layer3[i])\n",
    "    \n",
    "for i in range(20):\n",
    "    allLists.append(bias1[i])\n",
    "    \n",
    "for i in range(10):\n",
    "    allLists.append(bias2[i])\n",
    "    \n",
    "allLists.append(bias3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b505f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting allLists so that you can pause, quit, close the laptop mid way without worrying.\n",
    "allLists = np.asarray(allLists)\n",
    "with open('./trainings/allLists.npy', 'wb') as f:\n",
    "        np.save(f, allLists)\n",
    "\n",
    "# PS: even if you skip the export steps, make sure to keep doing the type conversions otherwise there will be bugs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now comes the prediction part! Whoo Hoooo!!!\n",
    "\n",
    "# Dataset\n",
    "\n",
    "datasetX = []\n",
    "datasetY = []\n",
    "\n",
    "# Writing this part for a more complex underlying function or a lesser complex model\n",
    "# timeSeries length = 200\n",
    "# strides = 10\n",
    "# target Index = 600\n",
    "\n",
    "samplesPerTrainable = 20\n",
    "stride = 10\n",
    "timeSerieslength = 200\n",
    "targetIndex = 1000\n",
    "\n",
    "for i in range(samplesPerTrainable):\n",
    "    for history in allLists:\n",
    "        datasetX.append(history[stride*i : stride*i + timeSerieslength])\n",
    "        datasetY.append(history[targetIndex])\n",
    "\n",
    "datasetX = np.asarray(datasetX)\n",
    "datasetY = np.asarray(datasetY)\n",
    "\n",
    "# Reshaping the input series Dataset to be able to train the \"Ultimate\" model\n",
    "datasetX = np.reshape(datasetX, (datasetX.shape[0], datasetX.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we are at the \"Ultimate\" Model step\n",
    "def createPredModel():\n",
    "    predictionModel = Sequential()\n",
    "    predictionModel.add(LSTM(2, input_shape = (timeSerieslength, 1), activation = 'relu'))\n",
    "    predictionModel.add(Dense(4, activation = 'relu'))\n",
    "    predictionModel.add(Dense(1, activation = 'relu'))\n",
    "    predictionModel.summary()\n",
    "    return predictionModel\n",
    "\n",
    "predictionModel = createPredModel()\n",
    "predictionModel.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f08564",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "history = predictionModel.fit(datasetX, datasetY, epochs = num_epochs, verbose = 1, callbacks = [cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am sleep deprived and my brain is no longer working, so kindly plot the loss graph here :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training on very large datasets, we have exported the weights\n",
    "impPredModel = createPredModel()\n",
    "impPredModel.get_weights()\n",
    "impPredModel.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec5a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, so after doing everything, this part is for the moment of truth... Rapid Training:\n",
    "\n",
    "rapidModel = createExpModel()\n",
    "rapidModel.compile(optimizer = 'adam', loss= 'mse', metrics = ['accuracy'])\n",
    "\n",
    "uselessModelPartiallyTrained = createExpModel()\n",
    "uselessModelPartiallyTrained.compile(optimizer = 'adam', loss= 'mse', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Training\n",
    "\n",
    "timeSerieslength = 200\n",
    "\n",
    "trackingArray = []\n",
    "batch_size = 20\n",
    "epochs = 1 # Changed epochs from 5 to 1\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0,timeSerieslength): # Training only for 200 time steps\n",
    "        uselessModelPartiallyTrained.fit(np.asarray(inpX[i*15:i*15+batch_size]),np.asarray(targets[i*15:i*15+batch_size]))\n",
    "        trackingArray.append(np.array(uselessModelPartiallyTrained.get_weights()))\n",
    "print(len(trackingArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trackingArray = np.asarray(trackingArray)\n",
    "print(trackingArray.shape)\n",
    "for i in range(6):\n",
    "    print(np.asarray(trackingArray[0][i]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = []\n",
    "layer2 = []\n",
    "for i in range(10*20):\n",
    "    layer1.append([])\n",
    "    layer2.append([])\n",
    "bias1 = []\n",
    "bias2 = []\n",
    "layer3 = []\n",
    "for i in range(10):\n",
    "    layer3.append([])\n",
    "    bias1.append([])\n",
    "    bias1.append([])\n",
    "    bias2.append([])\n",
    "bias3 = []\n",
    "for i in range(trackingArray.shape[0]):\n",
    "    for k in range(200):\n",
    "        layer1[k].append(trackingArray[i][0][int(k//20)][int(k%20)])\n",
    "        layer2[k].append(trackingArray[i][2][int(k//10)][int(k%10)])\n",
    "    for k in range(10):\n",
    "        layer3[k].append(trackingArray[i][4][k][0])\n",
    "        bias2[k].append(trackingArray[i][3][k])\n",
    "        bias1[2*k].append(trackingArray[i][1][2*k])\n",
    "        bias1[2*k+1].append(trackingArray[i][1][2*k+1])\n",
    "    bias3.append(trackingArray[i][5])\n",
    "\n",
    "layer1 = np.asarray(layer1)\n",
    "layer2 = np.asarray(layer2)\n",
    "layer3 = np.asarray(layer3)\n",
    "bias1 = np.asarray(bias1)\n",
    "bias2 = np.asarray(bias2)\n",
    "bias3 = np.asarray(bias3)\n",
    "bias3 = np.reshape(bias3, (bias3.shape[0]))\n",
    "\n",
    "print(layer1.shape)\n",
    "print(layer2.shape)\n",
    "print(layer3.shape)\n",
    "print(bias1.shape)\n",
    "print(bias2.shape)\n",
    "print(bias3.shape)\n",
    "\n",
    "# This look familiar?.... Imma punch you guys in the face if you answered \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the shape of the model just to keep our logic on track\n",
    "for i in range(6):\n",
    "    print(np.asarray(rapidModel.get_weights()[i]).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the \"Ultimate\" model to get the jumped weights we need\n",
    "predLayer1 = np.reshape(impPredModel(layer1), (10,20))\n",
    "predLayer2 = np.reshape(impPredModel(layer2), (20,10))\n",
    "predLayer3 = np.reshape(impPredModel(layer3), (10,1))\n",
    "predBias1 = np.reshape(impPredModel(bias1), (20))\n",
    "predBias2 = np.reshape(impPredModel(bias2), (10))\n",
    "predBias3 = np.asarray(impPredModel(np.reshape(bias3, (1,200,1))))\n",
    "\n",
    "print(predLayer1.shape)\n",
    "print(predLayer2.shape)\n",
    "print(predLayer3.shape)\n",
    "print(predBias1.shape)\n",
    "print(predBias2.shape)\n",
    "print(predBias3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "rapidModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b677d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = np.asarray(rapidModel.get_layer(\"dense_43\").get_weights())\n",
    "new_weights[0] = predLayer1\n",
    "new_weights[1] = predBias1\n",
    "rapidModel.get_layer(\"dense_43\").set_weights(new_weights)\n",
    "new_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3379a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = np.asarray(rapidModel.get_layer(\"dense_44\").get_weights())\n",
    "new_weights[0] = predLayer2\n",
    "new_weights[1] = predBias2\n",
    "rapidModel.get_layer(\"dense_44\").set_weights(new_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ced3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = np.asarray(rapidModel.get_layer(\"dense_45\").get_weights())\n",
    "new_weights[0] = predLayer3\n",
    "new_weights[1] = np.asarray([int(predBias3)])\n",
    "rapidModel.get_layer(\"dense_45\").set_weights(new_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40490d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "uselessModel = createExpModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20de211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapid Model's training skipped forward in time by 400 epochs\n",
    "print(\"\\nInput:\\n\\n\", inpX[150:165])\n",
    "print(\"\\n\\n\\nUseless Model:\\n\\n\",uselessModel.predict(inpX[150:165]))\n",
    "print(\"\\n\\n\\nUseless Model trained for 200 epochs:\\n\\n\",uselessModelPartiallyTrained.predict(inpX[150:165]))\n",
    "print(\"\\n\\n\\nRapid trainined:\\n\\n\",rapidModel.predict(inpX[150:165]))\n",
    "print(\"\\n\\n\\ntargeted values:\\n\\n\",calculate(inpX[150:165], biasUsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32998e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would love to say \"Hence Proved!\" but if the values do match, it's your efforts\n",
    "# The hyper parameter tuning for this is very tedious to figure out\n",
    "# So if you managed to get an optimization jump, KUDOS! if you didn't try again, it will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc65fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
